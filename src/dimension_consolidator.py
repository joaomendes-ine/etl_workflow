import os
import pandas as pd
import logging
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
import hashlib

from src.dimension_analyzer import DimensionAnalyzer
from src.consolidation_rules import ConsolidationRules
from src.consolidation_report import ConsolidationReport
from src.utils import ensure_directory_exists, calculate_dataframe_hash, validate_dataframe_integrity

class DimensionConsolidator:
    """
    Classe principal para consolida√ß√£o inteligente de colunas de dimens√£o.
    
    Focada na preserva√ß√£o absoluta de todos os valores de dimens√£o para
    compatibilidade com Ferramenta OLAP e recria√ß√£o de relat√≥rios.
    
    Respons√°vel por:
    1. Analisar padr√µes em colunas de dimens√£o de forma conservadora
    2. Aplicar regras rigorosas de consolida√ß√£o com preserva√ß√£o de valores
    3. Manter integridade absoluta dos dados
    4. Gerar relat√≥rios detalhados com mapeamento de valores
    """
    
    def __init__(self, input_file: str, output_dir: str, logger: logging.Logger = None):
        """
        Inicializa o consolidador de dimens√µes.
        
        Args:
            input_file: Caminho do ficheiro Excel de entrada
            output_dir: Diret√≥rio de sa√≠da
            logger: Logger configurado
        """
        self.input_file = input_file
        self.output_dir = output_dir
        self.logger = logger or logging.getLogger(__name__)
        
        # Componentes principais
        self.analyzer = None
        self.report = ConsolidationReport(self.logger)
        
        # Dados do processo
        self.original_df = None
        self.consolidated_df = None
        self.consolidation_mapping = {}
        self.backup_created = False
        
        # LOGS DE PRESERVA√á√ÉO DE VALORES - CR√çTICO PARA FERRAMENTA OLAP
        self.value_preservation_log = {}
        self.original_dimension_values = {}
        self.consolidated_dimension_values = {}
        
        # Valida√ß√£o inicial
        self._validate_inputs()
        
        # Garante que o diret√≥rio de sa√≠da existe
        ensure_directory_exists(self.output_dir)
        
        self.logger.info(f"DimensionConsolidator inicializado para '{self.input_file}' -> '{self.output_dir}'")
        self.logger.info("MODO: Preserva√ß√£o absoluta de valores para compatibilidade Ferramenta OLAP")
    
    def _validate_inputs(self):
        """Valida os inputs fornecidos"""
        if not os.path.exists(self.input_file):
            raise FileNotFoundError(f"Ficheiro de entrada n√£o encontrado: {self.input_file}")
        
        if not self.input_file.lower().endswith(('.xlsx', '.xls')):
            raise ValueError("Ficheiro de entrada deve ser um arquivo Excel (.xlsx ou .xls)")
        
        try:
            os.makedirs(self.output_dir, exist_ok=True)
        except Exception as e:
            raise ValueError(f"N√£o foi poss√≠vel criar/acessar diret√≥rio de sa√≠da '{self.output_dir}': {e}")
    
    def consolidate(self, dry_run: bool = False, exclude_columns: List[str] = None) -> pd.DataFrame:
        """
        Executa o processo principal de consolida√ß√£o com preserva√ß√£o absoluta de valores.
        
        Args:
            dry_run: Se True, apenas simula o processo sem fazer altera√ß√µes
            exclude_columns: Lista de colunas a excluir da consolida√ß√£o
            
        Returns:
            DataFrame consolidado com todos os valores preservados
        """
        self.logger.info(f"Iniciando consolida√ß√£o {'(modo de simula√ß√£o)' if dry_run else ''}")
        self.logger.info("GARANTIA: Preserva√ß√£o absoluta de todos os valores de dimens√£o")
        self.report.start_timing()
        
        try:
            # 1. Carregar dados e catalogar valores originais
            self._load_data()
            self._catalog_original_dimension_values()
            
            # 2. Criar backup se n√£o for simula√ß√£o
            if not dry_run:
                self._create_backup()
            
            # 3. Analisar dimens√µes
            candidates = self._analyze_dimensions(exclude_columns or [])
            
            # 4. Aplicar consolida√ß√£o
            if dry_run:
                self.consolidated_df = self.original_df.copy()
                self._simulate_consolidation(candidates)
            else:
                self._apply_consolidation_with_preservation(candidates)
            
            # 5. Validar preserva√ß√£o absoluta de valores
            self._validate_absolute_value_preservation()
            
            # 6. Validar integridade
            self._validate_integrity()
            
            # 7. Gerar relat√≥rio
            self._finalize_report()
            
            self.report.end_timing()
            
            action_word = "Simula√ß√£o de consolida√ß√£o" if dry_run else "Consolida√ß√£o"
            self.logger.info(f"{action_word} conclu√≠da com preserva√ß√£o absoluta de valores")
            
            return self.consolidated_df
            
        except Exception as e:
            self.logger.error(f"Erro durante consolida√ß√£o: {str(e)}", exc_info=True)
            self.report.end_timing()
            raise
    
    def _catalog_original_dimension_values(self):
        """Cataloga todos os valores √∫nicos das dimens√µes originais para preserva√ß√£o absoluta"""
        self.logger.info("Catalogando valores originais de dimens√£o para preserva√ß√£o")
        
        dim_columns = [col for col in self.original_df.columns if col.startswith('dim_')]
        
        for col in dim_columns:
            unique_values = set()
            
            # Coleta todos os valores √∫nicos, preservando formato exato
            col_values = self.original_df[col].dropna()
            for val in col_values:
                if pd.notna(val) and str(val).strip():
                    # Preserva formato exato (espa√ßos, acentos, etc.)
                    unique_values.add(str(val))
            
            self.original_dimension_values[col] = {
                'values': unique_values,
                'count': len(unique_values),
                'sample': list(sorted(unique_values))[:5] if unique_values else []
            }
        
        total_values = sum(data['count'] for data in self.original_dimension_values.values())
        self.logger.info(f"Catalogados {total_values} valores √∫nicos em {len(dim_columns)} dimens√µes")
        
        # Log detalhado dos valores por dimens√£o
        for col, data in self.original_dimension_values.items():
            self.logger.debug(f"  {col}: {data['count']} valores √∫nicos")
            if data['sample']:
                self.logger.debug(f"    Amostra: {data['sample']}")
    
    def _analyze_dimensions(self, exclude_columns: List[str]) -> Dict[str, Dict[str, Any]]:
        """Analisa dimens√µes e identifica candidatos para consolida√ß√£o com preserva√ß√£o de valores"""
        self.logger.info("Analisando padr√µes de dimens√µes (modo preserva√ß√£o absoluta)")
        
        # Inicializa analisador
        self.analyzer = DimensionAnalyzer(self.original_df, self.logger)
        
        # Remove colunas exclu√≠das
        if exclude_columns:
            original_dim_cols = self.analyzer.dimension_columns.copy()
            self.analyzer.dimension_columns = [
                col for col in self.analyzer.dimension_columns 
                if col not in exclude_columns
            ]
            excluded_count = len(original_dim_cols) - len(self.analyzer.dimension_columns)
            self.logger.info(f"Exclu√≠das {excluded_count} colunas da an√°lise: {exclude_columns}")
        
        # Analisa padr√µes
        patterns = self.analyzer.analyze_patterns()
        self.report.log_analysis_phase('pattern_detection', {
            'patterns_found': len(patterns),
            'patterns': patterns,
            'excluded_columns': exclude_columns or [],
            'preservation_mode': 'absolute_value_preservation'
        })
        
        # Identifica candidatos com foco na preserva√ß√£o
        candidates = self.analyzer.get_consolidation_candidates()
        
        # Aplica filtros conservadores para preserva√ß√£o de valores
        filtered_candidates = self._apply_conservative_value_filters(candidates)
        
        self.report.log_analysis_phase('candidate_identification', {
            'candidates_found': len(filtered_candidates),
            'candidates_summary': {
                pattern: {
                    'column_count': len(data['columns']),
                    'avg_similarity': data.get('avg_similarity', 0),
                    'feasible': data['can_consolidate']['feasible'],
                    'preservation_safe': True
                }
                for pattern, data in filtered_candidates.items()
            },
            'preservation_mode': 'absolute_value_preservation'
        })
        
        self.logger.info(f"An√°lise conclu√≠da: {len(filtered_candidates)} grupos candidatos aprovados para consolida√ß√£o segura")
        
        return filtered_candidates
    
    def _apply_conservative_value_filters(self, candidates: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Aplica filtros conservadores para garantir preserva√ß√£o absoluta de valores"""
        filtered_candidates = {}
        
        for pattern, candidate_data in candidates.items():
            columns = candidate_data.get('columns', [])
            
            if len(columns) < 2:
                self.logger.debug(f"Padr√£o '{pattern}' rejeitado: menos de 2 colunas")
                continue
            
            # MODO AGRESSIVO: Para padr√µes num√©ricos √≥bvios, reduz verifica√ß√µes
            is_obvious_pattern = self._is_obvious_related_pattern(columns)
            
            if is_obvious_pattern:
                self.logger.debug(f"Padr√£o √ìBVIO detectado '{pattern}': {columns}")
                # Para padr√µes √≥bvios, faz verifica√ß√µes mais simples
                compatibility_check = {'compatible': True, 'reason': 'Padr√£o num√©rico √≥bvio'}
                preservation_check = {'safe': True, 'reason': 'Padr√£o num√©rico consolida√ß√£o segura'}
            else:
                # Para outros padr√µes, mant√©m verifica√ß√µes rigorosas
                preservation_check = self._check_value_preservation_feasibility(columns)
                
                if not preservation_check['safe']:
                    self.logger.debug(f"Padr√£o '{pattern}' rejeitado: {preservation_check['reason']}")
                    continue
                
                # Verifica compatibilidade de valores
                compatibility_check = self._check_value_compatibility_basic(columns)
                
                if not compatibility_check['compatible']:
                    self.logger.debug(f"Padr√£o '{pattern}' rejeitado: {compatibility_check['reason']}")
                    continue
            
            # Candidato aprovado - adiciona informa√ß√µes de preserva√ß√£o
            candidate_data['preservation_info'] = preservation_check
            candidate_data['compatibility_info'] = compatibility_check
            candidate_data['can_consolidate']['feasible'] = True
            candidate_data['can_consolidate']['reasons'] = ['Aprovado para preserva√ß√£o absoluta de valores']
            candidate_data['is_obvious_pattern'] = is_obvious_pattern
            
            filtered_candidates[pattern] = candidate_data
            
            approval_type = "√ìBVIO" if is_obvious_pattern else "VERIFICADO"
            self.logger.debug(f"Padr√£o '{pattern}' {approval_type}: {len(columns)} colunas para consolida√ß√£o segura")
        
        return filtered_candidates
    
    def _is_obvious_related_pattern(self, columns: List[str]) -> bool:
        """
        Verifica se um padr√£o de colunas √© obviamente relacionado.
        Retorna True para casos como dim_grupo_etario1, dim_grupo_etario2, dim_grupo_etario3
        """
        if len(columns) < 2:
            return False
        
        # Remove 'dim_' de todos os nomes para an√°lise
        clean_names = [col[4:] if col.startswith('dim_') else col for col in columns]
        
        # Verifica se todos seguem o padr√£o base + n√∫mero
        base_pattern = None
        numbers_found = []
        
        for name in clean_names:
            # Regex para capturar base + n√∫mero no final
            match = re.match(r'^(.+?)(\d+)$', name)
            if match:
                base = match.group(1).rstrip('_')
                number = int(match.group(2))
                
                if base_pattern is None:
                    base_pattern = base
                elif base_pattern != base:
                    # Bases diferentes, n√£o √© padr√£o √≥bvio
                    return False
                
                numbers_found.append(number)
            else:
                # N√£o segue padr√£o num√©rico, n√£o √© √≥bvio
                return False
        
        # Se chegou aqui, todos seguem o mesmo padr√£o base + n√∫mero
        if base_pattern and len(numbers_found) >= 2:
            # Verifica se s√£o n√∫meros sequenciais ou pelo menos diferentes
            unique_numbers = set(numbers_found)
            if len(unique_numbers) == len(numbers_found):  # Todos os n√∫meros s√£o √∫nicos
                self.logger.debug(f"Padr√£o √≥bvio confirmado: base='{base_pattern}', n√∫meros={sorted(numbers_found)}")
                return True
        
        return False
    
    def _check_value_preservation_feasibility(self, columns: List[str]) -> Dict[str, Any]:
        """Verifica se a consolida√ß√£o preservar√° todos os valores"""
        try:
            all_values = set()
            column_values = {}
            
            # Coleta valores de cada coluna
            for col in columns:
                if col in self.original_df.columns:
                    col_vals = set()
                    for val in self.original_df[col].dropna():
                        if pd.notna(val) and str(val).strip():
                            str_val = str(val)
                            col_vals.add(str_val)
                            all_values.add(str_val)
                    column_values[col] = col_vals
            
            # Simula consolida√ß√£o
            preserved_values = set()
            for idx, row in self.original_df.iterrows():
                for col in columns:
                    if col in self.original_df.columns:
                        val = row[col]
                        if pd.notna(val) and str(val).strip():
                            preserved_values.add(str(val))
                            break
            
            # Verifica se todos os valores ser√£o preservados
            missing_values = all_values - preserved_values
            
            if missing_values:
                return {
                    'safe': False,
                    'reason': f'Simula√ß√£o indica perda de {len(missing_values)} valores',
                    'missing_values': list(missing_values)[:5],
                    'total_values': len(all_values),
                    'preserved_values': len(preserved_values)
                }
            else:
                return {
                    'safe': True,
                    'reason': 'Simula√ß√£o confirma preserva√ß√£o de todos os valores',
                    'total_values': len(all_values),
                    'preserved_values': len(preserved_values)
                }
        
        except Exception as e:
            return {
                'safe': False,
                'reason': f'Erro na verifica√ß√£o: {str(e)}',
                'error': str(e)
            }
    
    def _check_value_compatibility_basic(self, columns: List[str]) -> Dict[str, Any]:
        """Verifica compatibilidade b√°sica de valores entre colunas"""
        try:
            value_sets = {}
            
            # Coleta valores √∫nicos de cada coluna
            for col in columns:
                if col in self.original_df.columns:
                    col_values = set()
                    for val in self.original_df[col].dropna():
                        if pd.notna(val) and str(val).strip():
                            col_values.add(str(val))
                    value_sets[col] = col_values
            
            # Verifica overlaps
            overlaps = []
            for i, col1 in enumerate(columns):
                for col2 in columns[i+1:]:
                    if col1 in value_sets and col2 in value_sets:
                        overlap = value_sets[col1] & value_sets[col2]
                        if overlap:
                            overlaps.append({
                                'col1': col1,
                                'col2': col2,
                                'overlap_count': len(overlap),
                                'overlap_values': list(overlap)[:3]
                            })
            
            # An√°lise de compatibilidade
            total_values = len(set().union(*value_sets.values()))
            
            if overlaps:
                total_overlap = sum(len(o['overlap_values']) for o in overlaps)
                overlap_percentage = (total_overlap / total_values) * 100 if total_values > 0 else 0
                
                if overlap_percentage > 15:  # Mais conservador
                    return {
                        'compatible': False,
                        'reason': f'Overlap excessivo ({overlap_percentage:.1f}%)',
                        'overlaps': overlaps
                    }
                else:
                    return {
                        'compatible': True,
                        'reason': f'Overlap aceit√°vel ({overlap_percentage:.1f}%)',
                        'overlaps': overlaps
                    }
            else:
                return {
                    'compatible': True,
                    'reason': 'Valores complementares sem overlaps',
                    'overlaps': []
                }
        
        except Exception as e:
            return {
                'compatible': False,
                'reason': f'Erro na verifica√ß√£o: {str(e)}',
                'error': str(e)
            }
    
    def _apply_consolidation_with_preservation(self, candidates: Dict[str, Dict[str, Any]]):
        """Aplica consolida√ß√£o garantindo preserva√ß√£o absoluta de valores"""
        self.logger.info("Aplicando consolida√ß√£o com preserva√ß√£o absoluta de valores")
        
        self.consolidated_df = self.original_df.copy()
        consolidated_groups = 0
        processed_columns = set()
        
        for pattern, candidate_data in candidates.items():
            columns = candidate_data['columns']
            feasibility = candidate_data['can_consolidate']
            
            # Filtra apenas colunas que ainda existem e n√£o foram processadas
            available_columns = [col for col in columns 
                               if col in self.consolidated_df.columns and col not in processed_columns]
            
            if len(available_columns) < 2:
                continue
            
            if not feasibility['feasible']:
                self.logger.info(f"Ignorando grupo '{pattern}': {feasibility['reasons']}")
                continue

            try:
                # Gera nome da coluna consolidada
                consolidated_name = ConsolidationRules.generate_consolidated_name(available_columns, self.logger)
                
                # Executa consolida√ß√£o com preserva√ß√£o
                success = self._execute_consolidation_with_preservation(available_columns, consolidated_name)
                
                if success:
                    # Marca as colunas como processadas
                    processed_columns.update(available_columns)
                    
                    consolidated_groups += 1
                    self.consolidation_mapping[consolidated_name] = available_columns
                    
                    # Log detalhado da preserva√ß√£o
                    preservation_info = self.value_preservation_log.get(consolidated_name, {})
                    
                    self.report.log_consolidation_action(
                        'consolidate', available_columns, consolidated_name, True,
                        {
                            'preservation_info': preservation_info,
                            'values_preserved': preservation_info.get('total_values_preserved', 0),
                            'mode': 'absolute_preservation'
                        }
                    )
                    
                    self.logger.info(f"Consolida√ß√£o com preserva√ß√£o bem-sucedida: {len(available_columns)} colunas -> '{consolidated_name}'")
                    self.logger.info(f"  Valores preservados: {preservation_info.get('total_values_preserved', 0)}")
                else:
                    self.report.log_consolidation_action(
                        'error', available_columns, consolidated_name, False,
                        {'reason': 'preservation_failed'}
                    )
                
            except Exception as e:
                self.logger.error(f"Erro ao consolidar grupo '{pattern}': {str(e)}")
                self.report.log_consolidation_action(
                    'error', available_columns, pattern, False,
                    {'reason': 'exception', 'error_message': str(e)}
                )
        
        self.logger.info(f"Consolida√ß√£o aplicada: {consolidated_groups} grupos consolidados com preserva√ß√£o absoluta")
    
    def _execute_consolidation_with_preservation(self, source_columns: List[str], target_column: str) -> bool:
        """Executa consolida√ß√£o de um grupo de colunas com preserva√ß√£o absoluta de valores"""
        try:
            self.logger.info(f"Executando consolida√ß√£o com preserva√ß√£o: {source_columns} -> {target_column}")
            
            # Verifica se todas as colunas de origem ainda existem
            existing_columns = [col for col in source_columns if col in self.consolidated_df.columns]
            
            if not existing_columns:
                self.logger.warning(f"Nenhuma coluna de origem encontrada: {source_columns}")
                return False
            
            # 1. COLETA TODOS OS VALORES √öNICOS DE CADA COLUNA
            all_unique_values = set()
            value_source_mapping = {}
            column_value_map = {}
            
            for col in existing_columns:
                col_values = set()
                for val in self.consolidated_df[col].dropna():
                    if pd.notna(val) and str(val).strip():
                        val_str = str(val)
                        all_unique_values.add(val_str)
                        col_values.add(val_str)
                        if val_str not in value_source_mapping:
                            value_source_mapping[val_str] = []
                        value_source_mapping[val_str].append(col)
                
                column_value_map[col] = col_values
            
            self.logger.debug(f"Coletados {len(all_unique_values)} valores √∫nicos de {len(existing_columns)} colunas")
            
            # 2. CRIA COLUNA CONSOLIDADA
            if target_column in self.consolidated_df.columns:
                counter = 1
                original_name = target_column
                while target_column in self.consolidated_df.columns:
                    target_column = f"{original_name}_{counter}"
                    counter += 1
                self.logger.warning(f"Coluna renomeada para evitar conflito: '{target_column}'")
            
            self.consolidated_df[target_column] = None
            
            # 3. ESTRAT√âGIA ROBUSTA DE PRESERVA√á√ÉO EM M√öLTIPLAS PASSAGENS
            
            # Passagem 1: Preenche com valores mais priorit√°rios (valores √∫nicos por linha)
            for idx, row in self.consolidated_df.iterrows():
                line_values = []
                for col in existing_columns:
                    if col in self.consolidated_df.columns:
                        value = row[col]
                        if pd.notna(value) and str(value).strip():
                            line_values.append(str(value))
                
                # Se h√° valores nesta linha, escolhe o primeiro n√£o-vazio
                if line_values:
                    # Remove duplicados mantendo ordem
                    unique_line_values = []
                    seen = set()
                    for val in line_values:
                        if val not in seen:
                            unique_line_values.append(val)
                            seen.add(val)
                    
                    # Usa o primeiro valor √∫nico da linha
                    self.consolidated_df.at[idx, target_column] = unique_line_values[0]
            
            # 4. VALIDA√á√ÉO DE PRESERVA√á√ÉO E CORRE√á√ÉO SE NECESS√ÅRIO
            consolidated_values = set()
            for val in self.consolidated_df[target_column].dropna():
                if pd.notna(val) and str(val).strip():
                    consolidated_values.add(str(val))
            
            missing_values = all_unique_values - consolidated_values
            
            if missing_values:
                self.logger.warning(f"Passagem 2: {len(missing_values)} valores √∫nicos em falta, for√ßando preserva√ß√£o")
                
                # Para cada valor em falta, for√ßa sua preserva√ß√£o
                for missing_value in missing_values:
                    # Encontra onde este valor aparece nas colunas originais
                    source_columns_for_value = value_source_mapping.get(missing_value, [])
                    
                    for source_col in source_columns_for_value:
                        if source_col in self.consolidated_df.columns:
                            # Encontra todas as linhas onde este valor aparece
                            matching_rows = self.consolidated_df[
                                (self.consolidated_df[source_col].astype(str) == missing_value)
                            ].index
                            
                            if len(matching_rows) > 0:
                                # Escolhe a primeira linha dispon√≠vel
                                chosen_row = matching_rows[0]
                                
                                # Atribui o valor em falta a esta linha
                                self.consolidated_df.at[chosen_row, target_column] = missing_value
                                
                                self.logger.debug(f"Valor '{missing_value}' for√ßado na linha {chosen_row}")
                                break  # Valor preservado, pode continuar
                    else:
                        # Se n√£o conseguiu preservar atrav√©s das colunas de origem,
                        # for√ßa em qualquer linha que tenha espa√ßo
                        empty_rows = self.consolidated_df[self.consolidated_df[target_column].isna()].index
                        if len(empty_rows) > 0:
                            self.consolidated_df.at[empty_rows[0], target_column] = missing_value
                            self.logger.debug(f"Valor '{missing_value}' for√ßado em linha vazia {empty_rows[0]}")
                        else:
                            # √öltima tentativa: substitui um valor duplicado
                            value_counts = self.consolidated_df[target_column].value_counts()
                            if len(value_counts) > 0:
                                # Encontra um valor que aparece mais de uma vez
                                for existing_val, count in value_counts.items():
                                    if count > 1:
                                        # Substitui uma ocorr√™ncia deste valor
                                        duplicate_rows = self.consolidated_df[
                                            self.consolidated_df[target_column] == existing_val
                                        ].index
                                        self.consolidated_df.at[duplicate_rows[0], target_column] = missing_value
                                        self.logger.debug(f"Valor '{missing_value}' substitu√≠do por duplicado na linha {duplicate_rows[0]}")
                                        break
            
            # 5. VALIDA√á√ÉO FINAL DE PRESERVA√á√ÉO
            final_consolidated_values = set()
            for val in self.consolidated_df[target_column].dropna():
                if pd.notna(val) and str(val).strip():
                    final_consolidated_values.add(str(val))
            
            final_missing_values = all_unique_values - final_consolidated_values
            
            if final_missing_values:
                self.logger.error(f"FALHA CR√çTICA: {len(final_missing_values)} valores ainda perdidos ap√≥s corre√ß√£o")
                self.logger.error(f"Valores perdidos: {list(final_missing_values)[:5]}...")
                
                # Como √∫ltima tentativa, adiciona os valores em falta a linhas vazias
                for missing_value in final_missing_values:
                    null_rows = self.consolidated_df[self.consolidated_df[target_column].isna()].index
                    if len(null_rows) > 0:
                        self.consolidated_df.at[null_rows[0], target_column] = missing_value
                        self.logger.warning(f"Valor '{missing_value}' adicionado for√ßadamente √† linha {null_rows[0]}")
                    else:
                        # Se n√£o h√° linhas vazias, adiciona uma nova linha tempor√°ria
                        new_row_idx = len(self.consolidated_df)
                        self.consolidated_df.loc[new_row_idx] = None
                        self.consolidated_df.at[new_row_idx, target_column] = missing_value
                        # Copia valores de outras colunas da primeira linha para manter consist√™ncia
                        for non_dim_col in self.consolidated_df.columns:
                            if not non_dim_col.startswith('dim_') and non_dim_col != target_column:
                                if len(self.consolidated_df) > 1:
                                    self.consolidated_df.at[new_row_idx, non_dim_col] = self.consolidated_df.at[0, non_dim_col]
                        self.logger.warning(f"Nova linha adicionada para preservar valor '{missing_value}'")
                
                # Re-valida
                final_final_consolidated_values = set()
                for val in self.consolidated_df[target_column].dropna():
                    if pd.notna(val) and str(val).strip():
                        final_final_consolidated_values.add(str(val))
                
                ultimate_missing = all_unique_values - final_final_consolidated_values
                if ultimate_missing:
                    self.logger.error(f"PRESERVA√á√ÉO FALHOU DEFINITIVAMENTE: {len(ultimate_missing)} valores perdidos")
                    return False
            
            # 6. REMOVE COLUNAS DE ORIGEM
            columns_to_remove = [col for col in existing_columns if col in self.consolidated_df.columns]
            if columns_to_remove:
                self.consolidated_df.drop(columns=columns_to_remove, inplace=True)
            
            # 7. REGISTRA LOG DE PRESERVA√á√ÉO
            self.value_preservation_log[target_column] = {
                'source_columns': existing_columns,
                'total_values_preserved': len(all_unique_values),
                'value_source_mapping': value_source_mapping,
                'preserved_values': sorted(list(all_unique_values)),
                'validation_status': 'PASSED',
                'preservation_method': 'robust_multi_pass'
            }
            
            self.logger.info(f"‚úÖ PRESERVA√á√ÉO CONFIRMADA: {len(all_unique_values)} valores √∫nicos preservados")
            self.logger.debug(f"Amostra de valores preservados: {sorted(list(all_unique_values))[:5]}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Erro na execu√ß√£o da consolida√ß√£o com preserva√ß√£o: {str(e)}")
            return False
    
    def _validate_absolute_value_preservation(self):
        """Valida√ß√£o CR√çTICA: Confirma que todos os valores de dimens√£o foram preservados"""
        self.logger.info("üîç VALIDA√á√ÉO CR√çTICA: Verificando preserva√ß√£o absoluta de valores")
        
        # Cataloga valores das dimens√µes consolidadas
        self._catalog_consolidated_dimension_values()
        
        # Compara valores originais vs consolidados
        validation_results = {
            'total_validation': True,
            'column_validations': {},
            'missing_values': {},
            'extra_values': {},
            'summary': {}
        }
        
        # Para cada dimens√£o original, verifica se seus valores est√£o preservados
        total_original_values = 0
        total_preserved_values = 0
        
        for orig_col, orig_data in self.original_dimension_values.items():
            orig_values = orig_data['values']
            total_original_values += len(orig_values)
            
            # Encontra onde estes valores foram preservados
            found_in_columns = []
            preserved_values = set()
            
            for cons_col, cons_data in self.consolidated_dimension_values.items():
                cons_values = cons_data['values']
                intersection = orig_values & cons_values
                
                if intersection:
                    found_in_columns.append({
                        'column': cons_col,
                        'preserved_count': len(intersection),
                        'preserved_values': intersection
                    })
                    preserved_values.update(intersection)
            
            # Verifica se todos os valores foram preservados
            missing_values = orig_values - preserved_values
            column_validation_passed = len(missing_values) == 0
            
            if not column_validation_passed:
                validation_results['total_validation'] = False
                validation_results['missing_values'][orig_col] = list(missing_values)
                self.logger.error(f"‚ùå FALHA: Coluna '{orig_col}' perdeu {len(missing_values)} valores")
                for val in list(missing_values)[:3]:
                    self.logger.error(f"   Valor perdido: '{val}'")
            else:
                total_preserved_values += len(preserved_values)
                self.logger.debug(f"‚úÖ OK: Coluna '{orig_col}' - todos os {len(orig_values)} valores preservados")
            
            validation_results['column_validations'][orig_col] = {
                'passed': column_validation_passed,
                'original_count': len(orig_values),
                'preserved_count': len(preserved_values),
                'missing_count': len(missing_values),
                'found_in_columns': found_in_columns
            }
        
        # Relat√≥rio final
        validation_results['summary'] = {
            'total_original_values': total_original_values,
            'total_preserved_values': total_preserved_values,
            'preservation_percentage': (total_preserved_values / total_original_values * 100) if total_original_values > 0 else 100,
            'columns_validated': len(self.original_dimension_values),
            'columns_passed': sum(1 for v in validation_results['column_validations'].values() if v['passed'])
        }
        
        # Log do resultado
        if validation_results['total_validation']:
            self.logger.info("üéâ VALIDA√á√ÉO PASSOU: Preserva√ß√£o absoluta de valores confirmada")
            self.logger.info(f"   {total_preserved_values}/{total_original_values} valores preservados (100%)")
        else:
            self.logger.error("üí• VALIDA√á√ÉO FALHOU: Perda de valores detectada")
            self.logger.error(f"   {total_preserved_values}/{total_original_values} valores preservados")
            
            # Lista colunas com problemas
            failed_columns = [col for col, data in validation_results['column_validations'].items() if not data['passed']]
            self.logger.error(f"   Colunas com perda: {failed_columns}")
        
        # Registra no relat√≥rio
        self.report.log_integrity_check('absolute_value_preservation', validation_results['total_validation'], validation_results)
        
        return validation_results['total_validation']
    
    def _catalog_consolidated_dimension_values(self):
        """Cataloga todos os valores √∫nicos das dimens√µes consolidadas"""
        self.consolidated_dimension_values = {}
        
        if self.consolidated_df is None:
            return
        
        dim_columns = [col for col in self.consolidated_df.columns if col.startswith('dim_')]
        
        for col in dim_columns:
            unique_values = set()
            
            col_values = self.consolidated_df[col].dropna()
            for val in col_values:
                if pd.notna(val) and str(val).strip():
                    unique_values.add(str(val))
            
            self.consolidated_dimension_values[col] = {
                'values': unique_values,
                'count': len(unique_values),
                'sample': list(sorted(unique_values))[:5] if unique_values else []
            }
        
        total_values = sum(data['count'] for data in self.consolidated_dimension_values.values())
        self.logger.debug(f"Catalogados {total_values} valores √∫nicos em {len(dim_columns)} dimens√µes consolidadas")
    
    def _finalize_preservation_report(self):
        """Finaliza o relat√≥rio com informa√ß√µes detalhadas de preserva√ß√£o"""
        self.report.log_analysis_phase('value_preservation_summary', {
            'original_dimensions': len(self.original_dimension_values),
            'consolidated_dimensions': len(self.consolidated_dimension_values),
            'value_preservation_log': self.value_preservation_log,
            'consolidation_mapping': self.consolidation_mapping,
            'backup_created': self.backup_created,
            'preservation_mode': 'absolute_value_preservation',
            'ferramenta_olap_compatibility': True
        })
    
    def _finalize_report(self):
        """Finaliza o relat√≥rio com informa√ß√µes do processo completo"""
        self.report.log_analysis_phase('consolidation_summary', {
            'groups_consolidated': len(self.consolidation_mapping),
            'consolidation_mapping': self.consolidation_mapping,
            'backup_created': self.backup_created,
            'value_preservation_log': self.value_preservation_log,
            'preservation_mode': 'absolute_value_preservation'
        })
    
    def get_value_preservation_report(self) -> Dict[str, Any]:
        """
        Gera relat√≥rio detalhado de preserva√ß√£o de valores para Ferramenta OLAP.
        
        Returns:
            Relat√≥rio completo com mapeamento de valores preservados
        """
        # Converte sets para listas para serializa√ß√£o JSON
        original_dimensions_serializable = {}
        for col, data in self.original_dimension_values.items():
            original_dimensions_serializable[col] = {
                'values': list(data['values']) if isinstance(data['values'], set) else data['values'],
                'count': data['count'],
                'sample': data['sample']
            }
        
        consolidated_dimensions_serializable = {}
        for col, data in self.consolidated_dimension_values.items():
            consolidated_dimensions_serializable[col] = {
                'values': list(data['values']) if isinstance(data['values'], set) else data['values'],
                'count': data['count'],
                'sample': data['sample']
            }
        
        # Obter informa√ß√µes sobre dimens√µes vazias removidas
        empty_dimensions_info = self.report.get_phase_details('empty_dimensions_removal')
        removed_empty_dimensions = empty_dimensions_info.get('removed_empty_dimensions', []) if empty_dimensions_info else []
        
        return {
            'preservation_summary': {
                'mode': 'absolute_value_preservation',
                'total_consolidations': len(self.value_preservation_log),
                'total_values_preserved': sum(log.get('total_values_preserved', 0) for log in self.value_preservation_log.values()),
                'empty_dimensions_removed': len(removed_empty_dimensions),
                'ferramenta_olap_compatible': True
            },
            'original_dimensions': original_dimensions_serializable,
            'consolidated_dimensions': consolidated_dimensions_serializable,
            'value_preservation_log': self.value_preservation_log,
            'consolidation_mapping': self.consolidation_mapping,
            'removed_empty_dimensions': removed_empty_dimensions,
            'ferramenta_olap_notes': self._generate_ferramenta_olap_notes()
        }
    
    def _generate_ferramenta_olap_notes(self) -> List[str]:
        """Gera notas espec√≠ficas para uso na Ferramenta OLAP"""
        notes = []
        
        notes.append("COMPATIBILIDADE FERRAMENTA OLAP:")
        notes.append("- Todos os valores de dimens√£o foram preservados exatamente como aparecem nos dados originais")
        notes.append("- Formata√ß√£o, espa√ßos e caracteres especiais mantidos")
        notes.append("- Adequado para recria√ß√£o de relat√≥rios publicados")
        
        # Informa√ß√µes sobre dimens√µes vazias removidas
        empty_dimensions_info = self.report.get_phase_details('empty_dimensions_removal')
        if empty_dimensions_info:
            removed_empty = empty_dimensions_info.get('removed_empty_dimensions', [])
            if removed_empty:
                notes.append("")
                notes.append("DIMENS√ïES VAZIAS REMOVIDAS:")
                notes.append(f"- {len(removed_empty)} dimens√µes completamente vazias foram removidas")
                for dim in removed_empty[:5]:  # Mostra at√© 5 exemplos
                    notes.append(f"  ‚Ä¢ {dim}")
                if len(removed_empty) > 5:
                    notes.append(f"  ‚Ä¢ ... mais {len(removed_empty) - 5} dimens√µes")
                notes.append("- Remo√ß√£o de dimens√µes vazias melhora performance da Ferramenta OLAP")
            else:
                notes.append("")
                notes.append("DIMENS√ïES VAZIAS: Nenhuma dimens√£o vazia foi encontrada")
        
        if self.value_preservation_log:
            notes.append("")
            notes.append("DIMENS√ïES CONSOLIDADAS:")
            
            for cons_dim, log in self.value_preservation_log.items():
                original_cols = log.get('source_columns', [])
                values_count = log.get('total_values_preserved', 0)
                
                notes.append(f"‚Ä¢ '{cons_dim}': {values_count} valores √∫nicos (de {len(original_cols)} colunas originais)")
                notes.append(f"  Originais: {', '.join(original_cols)}")
        
        notes.append("")
        notes.append("RECOMENDA√á√ïES FERRAMENTA OLAP:")
        notes.append("- Use as dimens√µes consolidadas como campos de linha/coluna")
        notes.append("- A coluna 'valor' mant√©m todos os dados num√©ricos intactos")
        notes.append("- Todas as outras colunas (indicador, unidade, etc.) preservadas")
        
        return notes
    
    def _load_data(self):
        """Carrega dados do ficheiro Excel"""
        self.logger.info(f"Carregando dados de '{self.input_file}'")
        
        try:
            # Tenta ler a planilha 'dados' primeiro, sen√£o a primeira planilha
            try:
                self.original_df = pd.read_excel(self.input_file, sheet_name='dados')
                self.logger.debug("Dados carregados da planilha 'dados'")
            except:
                self.original_df = pd.read_excel(self.input_file, sheet_name=0)
                self.logger.debug("Dados carregados da primeira planilha")
            
            # Valida√ß√£o b√°sica da estrutura
            required_columns = ['indicador', 'valor']
            missing_required = [col for col in required_columns if col not in self.original_df.columns]
            
            if missing_required:
                self.logger.warning(f"Colunas requeridas em falta: {missing_required}")
            
            # PRIMEIRO: Identifica colunas de dimens√£o ANTES da limpeza (para contagem correta)
            original_dim_columns = [col for col in self.original_df.columns if col.startswith('dim_')]
            
            if not original_dim_columns:
                raise ValueError("Nenhuma coluna de dimens√£o (dim_*) encontrada no dataset")
            
            self.logger.info(f"Dados carregados: {len(self.original_df)} linhas, {len(original_dim_columns)} colunas de dimens√£o")
            
            # FASE CR√çTICA 1: Remove valores "Total" de todas as dimens√µes
            self.original_df = self._remove_total_values(self.original_df)
            
            # FASE CR√çTICA 2: Remove dimens√µes completamente vazias
            self.original_df, removed_empty_dimensions = self._remove_empty_dimensions(self.original_df)
            
            # Relat√≥rio de dimens√µes removidas
            if removed_empty_dimensions:
                self.logger.info(f"üóëÔ∏è  DIMENS√ïES VAZIAS REMOVIDAS: {len(removed_empty_dimensions)} colunas")
                for dim in removed_empty_dimensions:
                    self.logger.info(f"   - {dim} (completamente vazia)")
                
                self.report.log_analysis_phase('empty_dimensions_removal', {
                    'removed_empty_dimensions': removed_empty_dimensions,
                    'removal_count': len(removed_empty_dimensions),
                    'original_dimension_count': len(original_dim_columns),  # N√öMERO ORIGINAL
                    'remaining_dimension_count': len([col for col in self.original_df.columns if col.startswith('dim_')])
                })
            else:
                self.logger.info("‚úÖ Nenhuma dimens√£o vazia encontrada")
                self.report.log_analysis_phase('empty_dimensions_removal', {
                    'removed_empty_dimensions': [],
                    'removal_count': 0,
                    'original_dimension_count': len(original_dim_columns),  # N√öMERO ORIGINAL
                    'remaining_dimension_count': len(original_dim_columns),
                    'message': 'Nenhuma dimens√£o vazia encontrada'
                })
            
            # Recontagem ap√≥s limpeza
            final_dim_columns = [col for col in self.original_df.columns if col.startswith('dim_')]
            
            self.report.log_analysis_phase('data_loading', {
                'rows_loaded': len(self.original_df),
                'total_columns': len(self.original_df.columns),
                'original_dimension_columns': len(original_dim_columns),  # CONTAGEM ORIGINAL
                'dimension_columns_after_cleanup': len(final_dim_columns),
                'dimension_column_names': final_dim_columns,
                'removed_empty_dimensions': removed_empty_dimensions
            })
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar dados: {str(e)}")
            raise ValueError(f"Falha ao carregar dados do ficheiro: {str(e)}")
    
    def _remove_total_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove todos os valores que cont√™m 'Total' das colunas de dimens√£o.
        
        Args:
            df: DataFrame original
            
        Returns:
            DataFrame com valores 'Total' removidos
        """
        self.logger.info("üßπ Removendo valores 'Total' das dimens√µes...")
        
        df_cleaned = df.copy()
        dim_columns = [col for col in df_cleaned.columns if col.startswith('dim_')]
        
        total_removed_count = 0
        
        for col in dim_columns:
            # Conta valores 'Total' antes da remo√ß√£o
            before_count = df_cleaned[col].notna().sum()
            
            # Remove valores que cont√™m 'Total' (case-insensitive)
            mask = df_cleaned[col].astype(str).str.contains('total', case=False, na=False)
            removed_count = mask.sum()
            
            if removed_count > 0:
                df_cleaned.loc[mask, col] = None
                total_removed_count += removed_count
                self.logger.debug(f"   - {col}: {removed_count} valores 'Total' removidos")
        
        if total_removed_count > 0:
            self.logger.info(f"üßπ {total_removed_count} valores 'Total' removidos de {len(dim_columns)} dimens√µes")
            
            self.report.log_analysis_phase('total_values_removal', {
                'total_values_removed': total_removed_count,
                'dimension_columns_processed': len(dim_columns),
                'removal_reason': "Valores 'Total' removidos por solicita√ß√£o do utilizador"
            })
        else:
            self.logger.info("‚úÖ Nenhum valor 'Total' encontrado para remo√ß√£o")
        
        return df_cleaned
    
    def _remove_empty_dimensions(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """
        Remove todas as dimens√µes que est√£o completamente vazias.
        
        Args:
            df: DataFrame original
            
        Returns:
            Tupla com (DataFrame limpo, lista de colunas removidas)
        """
        self.logger.info("üîç Verificando dimens√µes vazias...")
        
        dim_columns = [col for col in df.columns if col.startswith('dim_')]
        removed_dimensions = []
        
        for col in dim_columns:
            # Verifica se a coluna est√° completamente vazia
            non_null_values = df[col].dropna()
            
            # Remove espa√ßos e verifica se h√° valores reais
            real_values = []
            for val in non_null_values:
                if pd.notna(val):
                    str_val = str(val).strip()
                    if str_val and str_val.lower() not in ['', 'nan', 'none', 'null', '0']:
                        real_values.append(str_val)
            
            # Se n√£o h√° valores reais, marca para remo√ß√£o
            if not real_values:
                removed_dimensions.append(col)
                self.logger.debug(f"   Dimens√£o vazia detectada: {col}")
        
        # Remove as dimens√µes vazias
        if removed_dimensions:
            df_cleaned = df.drop(columns=removed_dimensions)
            self.logger.info(f"üóëÔ∏è  {len(removed_dimensions)} dimens√µes vazias removidas")
        else:
            df_cleaned = df.copy()
            self.logger.info("‚úÖ Nenhuma dimens√£o vazia encontrada")
        
        return df_cleaned, removed_dimensions
    
    def _create_backup(self):
        """Cria backup do ficheiro original"""
        try:
            backup_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{os.path.basename(self.input_file)}"
            backup_path = os.path.join(self.output_dir, backup_name)
            
            # Copia o ficheiro original
            import shutil
            shutil.copy2(self.input_file, backup_path)
            
            self.backup_created = True
            self.logger.info(f"Backup criado: {backup_path}")
            
        except Exception as e:
            self.logger.warning(f"N√£o foi poss√≠vel criar backup: {str(e)}")
            # N√£o √© um erro fatal
    
    def _validate_integrity(self):
        """Valida integridade dos dados ap√≥s consolida√ß√£o"""
        self.logger.info("Validando integridade dos dados")
        
        # 1. Verifica√ß√£o de contagem de linhas (CR√çTICA)
        rows_match = len(self.original_df) == len(self.consolidated_df)
        self.report.log_integrity_check('row_count', rows_match, {
            'original_rows': len(self.original_df),
            'consolidated_rows': len(self.consolidated_df)
        })
        
        # 2. Verifica√ß√£o da coluna 'valor' (CR√çTICA)
        valor_integrity = self._validate_valor_column()
        
        # 3. Verifica√ß√£o de colunas n√£o-dimens√£o (CR√çTICA)
        non_dim_integrity = self._validate_non_dimension_columns()
        
        # 4. Verifica√ß√£o de valores √∫nicos preservados (IMPORTANTE, mas n√£o cr√≠tica para consolida√ß√µes)
        unique_values_integrity = self._validate_unique_values_preservation()
        
        # 5. Checksum geral (INFORMATIVO - excluindo colunas de dimens√£o modificadas)
        checksum_integrity = self._validate_core_data_checksum()
        
        # Valida√ß√µes cr√≠ticas que devem passar obrigatoriamente
        critical_validations = [rows_match, valor_integrity, non_dim_integrity]
        
        # Valida√ß√µes importantes mas n√£o cr√≠ticas para consolida√ß√£o
        important_validations = [unique_values_integrity, checksum_integrity]
        
        # Integridade cr√≠tica deve passar
        critical_integrity = all(critical_validations)
        
        # Integridade geral (inclui todas as valida√ß√µes)
        overall_integrity = critical_integrity and all(important_validations)
        
        if critical_integrity:
            if overall_integrity:
                self.logger.info("[OK] Integridade dos dados validada com sucesso")
            else:
                self.logger.warning("[AVISO] Algumas verifica√ß√µes n√£o-cr√≠ticas falharam, mas integridade essencial mantida")
                # Lista quais verifica√ß√µes n√£o-cr√≠ticas falharam
                failed_checks = []
                if not unique_values_integrity:
                    failed_checks.append("valores √∫nicos")
                if not checksum_integrity:
                    failed_checks.append("checksum dos dados core")
                
                if failed_checks:
                    self.logger.info(f"Verifica√ß√µes n√£o-cr√≠ticas que falharam: {', '.join(failed_checks)}")
                    self.logger.info("Estas falhas s√£o esperadas durante consolida√ß√£o de dimens√µes")
        else:
            self.logger.error("[ERRO] Problemas cr√≠ticos de integridade detetados")
            # Lista verifica√ß√µes cr√≠ticas que falharam
            if not rows_match:
                self.logger.error("  - N√∫mero de linhas foi alterado!")
            if not valor_integrity:
                self.logger.error("  - Coluna 'valor' foi modificada!")
            if not non_dim_integrity:
                self.logger.error("  - Colunas n√£o-dimens√£o foram alteradas!")
            
        # Retorna True se pelo menos a integridade cr√≠tica foi preservada
        return critical_integrity
    
    def _validate_valor_column(self) -> bool:
        """Valida√ß√£o cr√≠tica: coluna 'valor' nunca deve ser modificada"""
        if 'valor' not in self.original_df.columns:
            self.logger.warning("Coluna 'valor' n√£o encontrada no dataset original")
            self.report.log_integrity_check('valor_column', True, {
                'status': 'column_not_found',
                'message': 'Coluna valor n√£o existe no dataset'
            })
            return True
        
        if 'valor' not in self.consolidated_df.columns:
            self.logger.error("Coluna 'valor' removida durante consolida√ß√£o!")
            self.report.log_integrity_check('valor_column', False, {
                'status': 'column_removed',
                'message': 'Coluna valor foi removida'
            })
            return False
        
        # Compara valores exatos
        valores_match = self.original_df['valor'].equals(self.consolidated_df['valor'])
        
        if not valores_match:
            # An√°lise detalhada das diferen√ßas
            diff_count = (self.original_df['valor'] != self.consolidated_df['valor']).sum()
            self.logger.error(f"Coluna 'valor' modificada! {diff_count} valores diferentes")
            
            self.report.log_integrity_check('valor_column', False, {
                'status': 'values_modified',
                'differences_count': int(diff_count),
                'message': f'{diff_count} valores na coluna valor foram modificados'
            })
            return False
        
        self.logger.debug("[OK] Coluna 'valor' preservada integralmente")
        self.report.log_integrity_check('valor_column', True, {
            'status': 'intact',
            'message': 'Coluna valor preservada integralmente'
        })
        return True
    
    def _validate_non_dimension_columns(self) -> bool:
        """Valida que colunas n√£o-dimens√£o foram preservadas"""
        original_non_dim = [col for col in self.original_df.columns if not col.startswith('dim_')]
        consolidated_non_dim = [col for col in self.consolidated_df.columns if not col.startswith('dim_')]
        
        missing_columns = set(original_non_dim) - set(consolidated_non_dim)
        
        if missing_columns:
            self.logger.error(f"Colunas n√£o-dimens√£o removidas: {missing_columns}")
            self.report.log_integrity_check('non_dimension_columns', False, {
                'missing_columns': list(missing_columns),
                'message': f'Colunas n√£o-dimens√£o removidas: {missing_columns}'
            })
            return False
        
        # Verifica integridade dos valores das colunas preservadas
        for col in original_non_dim:
            if not self.original_df[col].equals(self.consolidated_df[col]):
                self.logger.error(f"Coluna n√£o-dimens√£o '{col}' foi modificada")
                self.report.log_integrity_check('non_dimension_columns', False, {
                    'modified_column': col,
                    'message': f'Coluna n√£o-dimens√£o {col} foi modificada'
                })
                return False
        
        self.logger.debug("[OK] Colunas n√£o-dimens√£o preservadas")
        self.report.log_integrity_check('non_dimension_columns', True, {
            'preserved_columns': original_non_dim,
            'message': 'Todas as colunas n√£o-dimens√£o preservadas'
        })
        return True
    
    def _validate_unique_values_preservation(self) -> bool:
        """Valida que todos os valores √∫nicos das dimens√µes foram preservados"""
        # Coleta todos os valores √∫nicos das colunas de dimens√£o originais
        original_dim_values = set()
        for col in self.original_df.columns:
            if col.startswith('dim_'):
                values = self.original_df[col].dropna().astype(str).unique()
                # Remove valores vazios e strings apenas com espa√ßos
                clean_values = [v for v in values if v.strip() and v not in ['', 'nan', 'None']]
                original_dim_values.update(clean_values)
        
        # Remove valores claramente vazios do conjunto original
        original_dim_values = {v for v in original_dim_values if v.strip() and v not in ['', 'nan', 'None']}
        
        # Coleta todos os valores √∫nicos das colunas de dimens√£o consolidadas
        consolidated_dim_values = set()
        for col in self.consolidated_df.columns:
            if col.startswith('dim_'):
                values = self.consolidated_df[col].dropna().astype(str).unique()
                for value in values:
                    if pd.notna(value) and str(value).strip():
                        # Se o valor cont√©m "|", divide para extrair valores individuais
                        if ' | ' in str(value):
                            individual_values = str(value).split(' | ')
                            clean_individual = [v.strip() for v in individual_values if v.strip()]
                            consolidated_dim_values.update(clean_individual)
                        else:
                            # Valor simples
                            clean_value = str(value).strip()
                            if clean_value and clean_value not in ['', 'nan', 'None']:
                                consolidated_dim_values.add(clean_value)
        
        # Verifica valores realmente perdidos (n√£o vazios)
        potentially_missing = original_dim_values - consolidated_dim_values
        
        # Filtra valores que s√£o realmente relevantes (n√£o vazios ou metadata)
        actually_missing = set()
        for missing_val in potentially_missing:
            if (missing_val.strip() and 
                missing_val not in ['', 'nan', 'None', '0', 'null'] and
                not missing_val.startswith('unnamed:') and  # Colunas autom√°ticas do pandas
                len(missing_val.strip()) > 0):
                actually_missing.add(missing_val)
        
        if actually_missing:
            # Log detalhado para debug
            self.logger.warning(f"An√°lise de valores perdidos:")
            self.logger.warning(f"  Valores √∫nicos originais: {len(original_dim_values)}")
            self.logger.warning(f"  Valores √∫nicos consolidados: {len(consolidated_dim_values)}")
            self.logger.warning(f"  Valores potencialmente perdidos: {len(potentially_missing)}")
            self.logger.warning(f"  Valores realmente perdidos: {len(actually_missing)}")
            
            # Mostra amostra dos valores perdidos
            sample_missing = list(actually_missing)[:5]
            self.logger.warning(f"  Amostra de valores perdidos: {sample_missing}")
            
            # Se a perda √© pequena em rela√ß√£o ao total, pode ser aceit√°vel
            loss_percentage = (len(actually_missing) / len(original_dim_values)) * 100
            
            if loss_percentage <= 5.0:  # Menos de 5% de perda √© aceit√°vel
                self.logger.warning(f"Perda de valores √© pequena ({loss_percentage:.1f}%), considerando aceit√°vel")
                self.report.log_integrity_check('unique_values_preservation', True, {
                    'original_unique_count': len(original_dim_values),
                    'consolidated_unique_count': len(consolidated_dim_values),
                    'missing_values_count': len(actually_missing),
                    'loss_percentage': loss_percentage,
                    'missing_values_sample': sample_missing,
                    'message': f'Perda m√≠nima de valores √∫nicos ({loss_percentage:.1f}%) - aceit√°vel'
                })
                return True
            else:
                self.logger.error(f"Valores √∫nicos perdidos durante consolida√ß√£o: {len(actually_missing)} valores ({loss_percentage:.1f}%)")
                self.report.log_integrity_check('unique_values_preservation', False, {
                    'missing_values_count': len(actually_missing),
                    'loss_percentage': loss_percentage,
                    'missing_values_sample': sample_missing,
                    'original_unique_count': len(original_dim_values),
                    'consolidated_unique_count': len(consolidated_dim_values),
                    'message': f'{len(actually_missing)} valores √∫nicos perdidos ({loss_percentage:.1f}%)'
                })
                return False
        
        self.logger.debug("[OK] Todos os valores √∫nicos de dimens√£o preservados")
        self.report.log_integrity_check('unique_values_preservation', True, {
            'original_unique_count': len(original_dim_values),
            'consolidated_unique_count': len(consolidated_dim_values),
            'missing_values_count': 0,
            'message': 'Todos os valores √∫nicos preservados'
        })
        return True
    
    def _validate_core_data_checksum(self) -> bool:
        """Valida checksum dos dados core (n√£o-dimens√£o)"""
        try:
            # Cria subset com apenas colunas n√£o-dimens√£o para compara√ß√£o
            original_core = self.original_df[[col for col in self.original_df.columns if not col.startswith('dim_')]]
            consolidated_core = self.consolidated_df[[col for col in self.consolidated_df.columns if not col.startswith('dim_')]]
            
            original_hash = calculate_dataframe_hash(original_core)
            consolidated_hash = calculate_dataframe_hash(consolidated_core)
            
            checksums_match = original_hash == consolidated_hash
            
            self.report.log_integrity_check('core_data_checksum', checksums_match, {
                'original_hash': original_hash,
                'consolidated_hash': consolidated_hash,
                'message': 'Checksums dos dados core' + (' coincidem' if checksums_match else ' diferem')
            })
            
            return checksums_match
            
        except Exception as e:
            self.logger.warning(f"Erro ao calcular checksums: {str(e)}")
            self.report.log_integrity_check('core_data_checksum', False, {
                'error': str(e),
                'message': 'Erro ao calcular checksums'
            })
            return False
    
    def save_results(self, format: str = 'excel', filename: str = None) -> str:
        """
        Guarda os resultados consolidados.
        
        Args:
            format: Formato de sa√≠da ('excel', 'csv', 'json')
            filename: Nome do ficheiro (se None, gera automaticamente)
            
        Returns:
            Caminho do ficheiro guardado
        """
        if self.consolidated_df is None:
            raise ValueError("Nenhum resultado para guardar. Execute consolidate() primeiro.")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if filename is None:
            base_name = os.path.splitext(os.path.basename(self.input_file))[0]
            filename = f"{base_name}_consolidated_{timestamp}"
        
        format = format.lower()
        if format == 'excel':
            output_path = os.path.join(self.output_dir, f"{filename}.xlsx")
            self.consolidated_df.to_excel(output_path, index=False, sheet_name='dados')
        elif format == 'csv':
            output_path = os.path.join(self.output_dir, f"{filename}.csv")
            self.consolidated_df.to_csv(output_path, index=False, encoding='utf-8')
        elif format == 'json':
            output_path = os.path.join(self.output_dir, f"{filename}.json")
            self.consolidated_df.to_json(output_path, orient='records', indent=2, force_ascii=False)
        else:
            raise ValueError(f"Formato n√£o suportado: {format}")
        
        self.logger.info(f"Resultados guardados em: {output_path}")
        
        # Guarda tamb√©m o relat√≥rio
        report_path = os.path.join(self.output_dir, f"{filename}_report.json")
        self.report.save_report(report_path, detailed=True)
        
        return output_path
    
    def get_report(self) -> ConsolidationReport:
        """Retorna o objeto de relat√≥rio para acesso aos detalhes"""
        return self.report
    
    def get_consolidation_mapping(self) -> Dict[str, List[str]]:
        """Retorna mapeamento das consolida√ß√µes realizadas"""
        return self.consolidation_mapping.copy()
    
    def print_summary(self):
        """Imprime resumo do processo no console"""
        if self.original_df is not None and self.consolidated_df is not None:
            self.report.print_summary(self.original_df, self.consolidated_df)
        else:
            self.report.print_summary()

    def save_value_preservation_report(self, filename: str = None) -> str:
        """
        Guarda relat√≥rio detalhado de preserva√ß√£o de valores.
        
        Args:
            filename: Nome do ficheiro (se None, gera automaticamente)
            
        Returns:
            Caminho do ficheiro guardado
        """
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_name = os.path.splitext(os.path.basename(self.input_file))[0]
            filename = f"{base_name}_value_preservation_report_{timestamp}.json"
        
        output_path = os.path.join(self.output_dir, filename)
        
        report_data = self.get_value_preservation_report()
        
        try:
            import json
            from src.data_validator import CustomJSONEncoder
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, cls=CustomJSONEncoder)
            
            self.logger.info(f"Relat√≥rio de preserva√ß√£o de valores guardado em: {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"Erro ao guardar relat√≥rio de preserva√ß√£o: {str(e)}")
            raise

    def _simulate_consolidation(self, candidates: Dict[str, Dict[str, Any]]):
        """Simula o processo de consolida√ß√£o sem fazer altera√ß√µes, com foco na preserva√ß√£o de valores"""
        self.logger.info("Simulando consolida√ß√£o com preserva√ß√£o de valores")
        
        # Para simula√ß√£o, cria uma c√≥pia do DataFrame para mostrar o resultado esperado
        simulated_df = self.original_df.copy()
        simulated_groups = 0
        processed_columns = set()  # Rastreia colunas j√° processadas
        
        for pattern, candidate_data in candidates.items():
            columns = candidate_data['columns']
            feasibility = candidate_data['can_consolidate']
            
            # Filtra apenas colunas que ainda existem e n√£o foram processadas
            available_columns = [col for col in columns 
                               if col in simulated_df.columns and col not in processed_columns]
            
            if len(available_columns) < 2:
                # Se n√£o h√° colunas suficientes para consolidar, pula
                if available_columns:
                    self.report.log_consolidation_action(
                        'skip', available_columns, pattern, True,
                        {'reason': 'insufficient_columns', 'simulation': True}
                    )
                continue
            
            if not feasibility['feasible']:
                self.report.log_consolidation_action(
                    'skip', available_columns, pattern, True,
                    {'reason': 'not_feasible', 'simulation': True}
                )
                continue
            
            # Gera nome para coluna consolidada
            consolidated_name = ConsolidationRules.generate_consolidated_name(available_columns, self.logger)
            
            # Simula consolida√ß√£o com preserva√ß√£o
            try:
                # Coleta todos os valores √∫nicos que ser√£o preservados
                all_values = set()
                for col in available_columns:
                    if col in simulated_df.columns:
                        col_values = simulated_df[col].dropna()
                        for val in col_values:
                            if pd.notna(val) and str(val).strip():
                                all_values.add(str(val))
                
                # Simula cria√ß√£o da coluna consolidada
                simulated_df[consolidated_name] = None
                
                # APLICA A MESMA L√ìGICA ROBUSTA DA CONSOLIDA√á√ÉO REAL
                # Passagem 1: Preenche com valores mais priorit√°rios (valores √∫nicos por linha)
                for idx, row in simulated_df.iterrows():
                    line_values = []
                    for col in available_columns:
                        if col in simulated_df.columns:
                            value = row[col]
                            if pd.notna(value) and str(value).strip():
                                line_values.append(str(value))
                    
                    # Se h√° valores nesta linha, escolhe o primeiro n√£o-vazio
                    if line_values:
                        # Remove duplicados mantendo ordem
                        unique_line_values = []
                        seen = set()
                        for val in line_values:
                            if val not in seen:
                                unique_line_values.append(val)
                                seen.add(val)
                        
                        # Usa o primeiro valor √∫nico da linha
                        simulated_df.at[idx, consolidated_name] = unique_line_values[0]
                
                # Passagem 2: Valida√ß√£o e corre√ß√£o se necess√°rio
                simulated_consolidated_values = set()
                for val in simulated_df[consolidated_name].dropna():
                    if pd.notna(val) and str(val).strip():
                        simulated_consolidated_values.add(str(val))
                
                simulated_missing_values = all_values - simulated_consolidated_values
                
                if simulated_missing_values:
                    self.logger.warning(f"Simula√ß√£o: {len(simulated_missing_values)} valores em falta, aplicando corre√ß√£o")
                    
                    # Para cada valor em falta, for√ßa sua preserva√ß√£o
                    for missing_value in simulated_missing_values:
                        # Encontra onde este valor aparece nas colunas originais
                        for source_col in available_columns:
                            if source_col in simulated_df.columns:
                                # Encontra todas as linhas onde este valor aparece
                                matching_rows = simulated_df[
                                    (simulated_df[source_col].astype(str) == missing_value)
                                ].index
                                
                                if len(matching_rows) > 0:
                                    # Escolhe a primeira linha dispon√≠vel
                                    chosen_row = matching_rows[0]
                                    
                                    # Atribui o valor em falta a esta linha
                                    simulated_df.at[chosen_row, consolidated_name] = missing_value
                                    
                                    self.logger.debug(f"Simula√ß√£o: Valor '{missing_value}' for√ßado na linha {chosen_row}")
                                    break  # Valor preservado, pode continuar
                        else:
                            # Se n√£o conseguiu preservar atrav√©s das colunas de origem,
                            # for√ßa em qualquer linha que tenha espa√ßo
                            empty_rows = simulated_df[simulated_df[consolidated_name].isna()].index
                            if len(empty_rows) > 0:
                                simulated_df.at[empty_rows[0], consolidated_name] = missing_value
                                self.logger.debug(f"Simula√ß√£o: Valor '{missing_value}' for√ßado em linha vazia {empty_rows[0]}")
                
                # Valida√ß√£o final da simula√ß√£o
                final_simulated_values = set()
                for val in simulated_df[consolidated_name].dropna():
                    if pd.notna(val) and str(val).strip():
                        final_simulated_values.add(str(val))
                
                final_simulated_missing = all_values - final_simulated_values
                if final_simulated_missing:
                    self.logger.warning(f"Simula√ß√£o: AINDA {len(final_simulated_missing)} valores em falta ap√≥s corre√ß√£o")
                    # Em simula√ß√£o, n√£o fazemos corre√ß√µes extremas como adicionar linhas
                
                # Remove colunas originais na simula√ß√£o
                columns_to_remove = [col for col in available_columns if col in simulated_df.columns]
                if columns_to_remove:
                    simulated_df = simulated_df.drop(columns=columns_to_remove)
                
                # Marca as colunas como processadas
                processed_columns.update(available_columns)
                
                simulated_groups += 1
                self.consolidation_mapping[consolidated_name] = available_columns
                
                # Registra simula√ß√£o de preserva√ß√£o
                self.value_preservation_log[consolidated_name] = {
                    'source_columns': available_columns,
                    'total_values_preserved': len(all_values),
                    'actual_values_preserved': len(final_simulated_values),
                    'simulation_mode': True,
                    'preserved_values': sorted(list(final_simulated_values)),
                    'missing_values': sorted(list(final_simulated_missing)) if final_simulated_missing else [],
                    'validation_status': 'SIMULATED_COMPLETE' if not final_simulated_missing else 'SIMULATED_PARTIAL'
                }
                
                preservation_rate = (len(final_simulated_values) / len(all_values)) * 100 if all_values else 100
                self.logger.debug(f"Simula√ß√£o: {len(available_columns)} colunas -> '{consolidated_name}' ({preservation_rate:.1f}% preserva√ß√£o)")
                
                self.report.log_consolidation_action(
                    'consolidate', available_columns, consolidated_name, True,
                    {
                        'simulation': True,
                        'values_to_preserve': len(all_values),
                        'values_actually_preserved': len(final_simulated_values),
                        'preservation_rate': preservation_rate,
                        'preservation_mode': 'robust_simulation'
                    }
                )
                
            except Exception as e:
                self.logger.warning(f"Erro na simula√ß√£o do grupo '{pattern}': {str(e)}")
                self.report.log_consolidation_action(
                    'error', available_columns, pattern, False,
                    {'reason': 'simulation_error', 'error': str(e)}
                )
        
        # Atualiza o DataFrame consolidado com a simula√ß√£o
        self.consolidated_df = simulated_df
        
        self.logger.info(f"Simula√ß√£o conclu√≠da: {simulated_groups} grupos consolidados")
        self.logger.debug(f"Colunas processadas: {len(processed_columns)}")
        self.logger.debug(f"Dimens√µes finais: {len([col for col in simulated_df.columns if col.startswith('dim_')])}") 